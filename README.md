# Deep-Learning

This repo includes all the practicals from my Deep Learning lab, implemented using Python, TensorFlow, Keras, NumPy, and a sprinkle of patience. Dive in to explore neural networks from scratch, classification tasks, optimization experiments, LSTMs, and even some autoencoder wizardry. ğŸ§™â€â™‚ï¸ğŸ“‰

---

## ğŸ“‚ Contents

### 1. ğŸ“‰ Insurance Cost Prediction using ANN
- Performed EDA to understand the dataset.
- Built baseline models using:
  - Perceptron
  - Deep Neural Networks (DNNs)
- Played with hyperparameters:
  - Epochs
  - Batch Size
  - Learning Rates
- Optimizers tested:
  - SGD
  - Momentum
  - Nesterov Accelerated Gradient
- Evaluated using metrics like MSE, MAE.
- âœ… Saved the trained ANN model.
- ğŸ” Comparative analysis of results included.

---

### 2. ğŸª· Perceptron Classifier on Iris Dataset
- Used the classic Iris dataset ğŸŒ¸.
- Focused on binary classification: `Setosa` vs `Versicolor`.
- Selected features: `Sepal Length` & `Sepal Width`.
- Implemented the **Perceptron Learning Algorithm** from scratch.
- Visualized the decision boundary.
- Accuracy evaluated on test set.

---

### 3. ğŸ—ï¸ Feedforward Neural Network from Scratch
- **Task A:** Binary Classification using `make_moons` / `make_circles`.
  - Activation: Sigmoid
  - Loss: Mean Squared Error
- **Task B:** Multiclass Classification
  - Network: 4-3-4-3 (input to output)
  - Activation: Sigmoid (hidden), Softmax (output)
  - Loss: Binary Cross Entropy (yes, for multiclassâ€”educational choice!)
- Built using only NumPy â€” no high-level libraries. We raw-coded it. ğŸ˜¤

---

### 4. ğŸ§± Multiclass Neural Net w/ ReLU + Softmax
- Architecture: 4 input â†’ 3 hidden â†’ 4 hidden â†’ 3 output
- ReLU used in hidden layers, Softmax in output
- Loss: Binary Cross Entropy
- Implemented optimizers:
  - SGD
  - Momentum
  - NAG
- Evaluated and compared the performance.

---

### 5. ğŸ’‰ Diabetes Prediction using ANN
- Performed thorough EDA.
- Trained ANN on diabetes dataset.
- Regularization Techniques applied:
  - L1, L2
  - Dropout
  - Early Stopping
- Tracked training using runtime chart â±ï¸.
- Comparative analysis included.
- âœ… Model saved for future diagnosis ğŸ¤–

---

### 6. ğŸ“ CNN & Dense Backprop from Scratch + Fruit Classification
- **Part A:** Wrote custom forward and backward pass for a dense network. No cheating.
- **Part B:** Fruit Classification using CNN
  - Dataset: [`utkarshsaxenadn/fruits-classification`](https://www.kaggle.com/datasets/utkarshsaxenadn/fruits-classification)
  - Achieved ğŸŒ vs ğŸ vs ğŸ‡ greatness.
  - Compared custom and pre-trained models.

---

### 7. ğŸ“ˆ LSTM for Stock Price Prediction (Google)
- Task: Predict next 15-day opening prices.
- Dataset preprocessed with 75/25 split, timestamp = 50.
- 4 stacked LSTM layers w/ 50 neurons each.
- Loss Function: Mean Squared Error
- Results visualized ğŸ“Š and plotted like real traders do (minus the losses).

---

### 8. ğŸ” Autoencoder for MNIST Digit Regeneration
- Used 784 â†’ 128 â†’ 64 â†’ 32 (encoder)
- Reconstructed back using decoder.
- Dataset: MNIST (handwritten digits)
- Model: Fully connected Autoencoder
- Output quality: Surprisingly crisp for a compressed code size of 32!
- Loss: Binary Cross Entropy

---

## ğŸ› ï¸ Tools & Libraries Used
- Python ğŸ
- TensorFlow / Keras
- NumPy / Pandas
- Matplotlib / Seaborn
- scikit-learn
- Jupyter Notebooks (aka code playgrounds)

---

## ğŸ“Œ How to Run
```bash
git clone https://github.com/yourusername/deep-learning-practicals.git
cd deep-learning-practicals
# Open desired notebook and run using Jupyter/Colab
