# -*- coding: utf-8 -*-
"""A1_13_DL-P2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m_LrFNwiT3x-YjdBVn8kYu6bmhAJQEjY

### Name: Radhika Shukla
### Roll No: 13
### Batch: A1
### Practical 2

### AIM: Design and implement a Python program to build a binary classifier using the Perceptron Learning Algorithm. The Iris dataset  will  be used to train and test the Perceptron Learning Algorithm. Focus on two classes of the Iris dataset (e.g., "Iris-setosa" and "Iris-versicolor") to create a binary classification problem. Use only two features (e.g., "sepal length" and "sepal width") for simplicity and visualization. Measure performance using Accuracy on the test set. Also Visualize the learned decision boundary.

oundary.ndary.
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

file_path = 'Iris.csv'
iris_data = pd.read_csv(file_path)

binary_classes = iris_data[iris_data['Species'].isin(['Iris-setosa', 'Iris-versicolor'])]
X = binary_classes[['SepalLengthCm', 'SepalWidthCm']].values
y = (binary_classes['Species'] == 'Iris-setosa').astype(int).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class Perceptron:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.n_iterations):
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_predicted = self._activation_function(linear_output)

                update = self.learning_rate * (y[idx] - y_predicted)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return self._activation_function(linear_output)

    def _activation_function(self, x):
        return np.where(x >= 0, 1, 0)

perceptron = Perceptron(learning_rate=0.01, n_iterations=1000)
perceptron.fit(X_train, y_train)

y_pred = perceptron.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))
    grid = np.c_[xx.ravel(), yy.ravel()]
    predictions = model.predict(grid).reshape(xx.shape)

    plt.contourf(xx, yy, predictions, alpha=0.8, cmap=plt.cm.Paired)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.Paired)
    plt.xlabel("Sepal Length")
    plt.ylabel("Sepal Width")
    plt.title("Decision Boundary")
    plt.show()

plot_decision_boundary(X, y, perceptron)

print("Accuracy on test set:", accuracy)